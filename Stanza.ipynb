{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanza is a collection of accurate and efficient tools for many human languages in one place.Starting from raw text to syntactic analysis and entity recognition, Stanza brings state-of-the-art NLP models to languages of your choosing.\n",
    "Stanza is a Python natural language analysis package. It contains tools, which can be used in a pipeline, to convert a string containing human language text into lists of sentences and words, to generate base forms of those words, their parts of speech and morphological features, to give a syntactic structure dependency parse, and to recognize named entities. The toolkit is designed to be parallel among more than 60 languages, using the Universal Dependencies formalism.\n",
    "\n",
    "Native Python implementation requiring minimal efforts to set up; Full neural network pipeline for robust text analytics, including tokenization, multi-word token (MWT) expansion, lemmatization, part-of-speech (POS) and morphological features tagging, dependency parsing, and named entity recognition; Pretrained neural models supporting 66 (human) languages; A stable, officially maintained Python interface to CoreNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.2-py3-none-any.whl (282 kB)\n",
      "Collecting torch>=1.3.0\n",
      "  Downloading torch-1.8.1-cp38-cp38-win_amd64.whl (190.5 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\piyush.pathak\\anaconda3\\lib\\site-packages (from stanza) (4.56.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\piyush.pathak\\anaconda3\\lib\\site-packages (from stanza) (3.13.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\piyush.pathak\\anaconda3\\lib\\site-packages (from stanza) (1.20.1)\n",
      "Requirement already satisfied: requests in c:\\users\\piyush.pathak\\anaconda3\\lib\\site-packages (from stanza) (2.24.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\piyush.pathak\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (3.7.4.2)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\piyush.pathak\\anaconda3\\lib\\site-packages (from protobuf->stanza) (1.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\piyush.pathak\\anaconda3\\lib\\site-packages (from protobuf->stanza) (49.2.0.post20200714)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\piyush.pathak\\anaconda3\\lib\\site-packages (from requests->stanza) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\piyush.pathak\\anaconda3\\lib\\site-packages (from requests->stanza) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\piyush.pathak\\anaconda3\\lib\\site-packages (from requests->stanza) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\piyush.pathak\\anaconda3\\lib\\site-packages (from requests->stanza) (3.0.4)\n",
      "Installing collected packages: torch, stanza\n",
      "Successfully installed stanza-1.2 torch-1.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 16.1MB/s]\n",
      "2021-03-29 13:20:21 INFO: Downloading default packages for language: en (English)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/default.zip: 100%|█████| 411M/411M [01:52<00:00, 3.65MB/s]\n",
      "2021-03-29 13:22:25 INFO: Finished downloading models and saved to C:\\Users\\piyush.pathak\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-29 13:22:25 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-03-29 13:22:25 INFO: Use device: cpu\n",
      "2021-03-29 13:22:25 INFO: Loading: tokenize\n",
      "2021-03-29 13:22:25 INFO: Loading: pos\n",
      "2021-03-29 13:22:26 INFO: Loading: lemma\n",
      "2021-03-29 13:22:26 INFO: Loading: depparse\n",
      "2021-03-29 13:22:26 INFO: Loading: sentiment\n",
      "2021-03-29 13:22:27 INFO: Loading: ner\n",
      "2021-03-29 13:22:28 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"Akshay\",\n",
      "      \"lemma\": \"Akshay\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"xpos\": \"NNP\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"misc\": \"start_char=0|end_char=6\",\n",
      "      \"ner\": \"S-PERSON\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"is\",\n",
      "      \"lemma\": \"be\",\n",
      "      \"upos\": \"AUX\",\n",
      "      \"xpos\": \"VBZ\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"aux\",\n",
      "      \"misc\": \"start_char=7|end_char=9\",\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"teaching\",\n",
      "      \"lemma\": \"teach\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBG\",\n",
      "      \"feats\": \"Tense=Pres|VerbForm=Part\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"misc\": \"start_char=10|end_char=18\",\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"Stanza\",\n",
      "      \"lemma\": \"Stanza\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"xpos\": \"NNP\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"misc\": \"start_char=19|end_char=25\",\n",
      "      \"ner\": \"S-ORG\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"library\",\n",
      "      \"lemma\": \"library\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"obj\",\n",
      "      \"misc\": \"start_char=26|end_char=33\",\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \".\",\n",
      "      \"lemma\": \".\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \".\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"misc\": \"start_char=33|end_char=34\",\n",
      "      \"ner\": \"O\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Akshay is teaching Stanza library.\")\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{\n",
       "   \"text\": \"Akshay\",\n",
       "   \"type\": \"PERSON\",\n",
       "   \"start_char\": 0,\n",
       "   \"end_char\": 6\n",
       " },\n",
       " {\n",
       "   \"text\": \"Stanza\",\n",
       "   \"type\": \"ORG\",\n",
       "   \"start_char\": 19,\n",
       "   \"end_char\": 25\n",
       " }]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akshay Akshay NNP\n",
      "is be VBZ\n",
      "teaching teach VBG\n",
      "Stanza Stanza NNP\n",
      "library library NN\n",
      ". . .\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(word.text, word.lemma, word.xpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 8.38MB/s]\n",
      "2021-03-29 13:22:29 WARNING: Can not find tokenize: gsd from official model list. Ignoring it.\n",
      "2021-03-29 13:22:29 WARNING: Can not find pos: hdt from official model list. Ignoring it.\n",
      "2021-03-29 13:22:29 INFO: Downloading these customized packages for language: en (English)...\n",
      "==============================\n",
      "| Processor       | Package  |\n",
      "------------------------------\n",
      "| lemma           | combined |\n",
      "| ner             | conll03  |\n",
      "| forward_charlm  | 1billion |\n",
      "| backward_charlm | 1billion |\n",
      "==============================\n",
      "\n",
      "2021-03-29 13:22:29 INFO: File exists: C:\\Users\\piyush.pathak\\stanza_resources\\en\\lemma\\combined.pt.\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/ner/conll03.pt: 100%|█| 80.8M/80.8M [00:56<00:00, 1.43MB/s\n",
      "2021-03-29 13:23:28 INFO: File exists: C:\\Users\\piyush.pathak\\stanza_resources\\en\\forward_charlm\\1billion.pt.\n",
      "2021-03-29 13:23:28 INFO: File exists: C:\\Users\\piyush.pathak\\stanza_resources\\en\\backward_charlm\\1billion.pt.\n",
      "2021-03-29 13:23:28 INFO: Finished downloading models and saved to C:\\Users\\piyush.pathak\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "processor_dict = {\n",
    "    'tokenize': 'gsd', \n",
    "    'pos': 'hdt', \n",
    "    'ner': 'conll03', \n",
    "    'lemma': 'default'\n",
    "}\n",
    "stanza.download('en', processors=processor_dict, package=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-29 13:24:32 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2021-03-29 13:24:32 INFO: Use device: cpu\n",
      "2021-03-29 13:24:32 INFO: Loading: tokenize\n",
      "2021-03-29 13:24:32 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: Akshay\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: teaching\n",
      "id: (4,)\ttext: Stanza\n",
      "id: (5,)\ttext: .\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: Stanza\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: the\n",
      "id: (4,)\ttext: next\n",
      "id: (5,)\ttext: revolution\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "doc = nlp('Akshay is teaching Stanza.Stanza is the next revolution')\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-29 13:25:00 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2021-03-29 13:25:00 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2021-03-29 13:25:00 INFO: Use device: cpu\n",
      "2021-03-29 13:25:00 INFO: Loading: tokenize\n",
      "2021-03-29 13:25:00 INFO: Loading: pos\n",
      "2021-03-29 13:25:00 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Barack\tupos: PROPN\txpos: NNP\tfeats: Number=Sing\n",
      "word: Obama\tupos: PROPN\txpos: NNP\tfeats: Number=Sing\n",
      "word: was\tupos: AUX\txpos: VBD\tfeats: Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "word: born\tupos: VERB\txpos: VBN\tfeats: Tense=Past|VerbForm=Part|Voice=Pass\n",
      "word: in\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: Hawaii\tupos: PROPN\txpos: NNP\tfeats: Number=Sing\n",
      "word: .\tupos: PUNCT\txpos: .\tfeats: _\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos')\n",
    "doc = nlp('Barack Obama was born in Hawaii.')\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-29 13:25:24 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| sentiment | sstplus  |\n",
      "========================\n",
      "\n",
      "2021-03-29 13:25:24 INFO: Use device: cpu\n",
      "2021-03-29 13:25:24 INFO: Loading: tokenize\n",
      "2021-03-29 13:25:24 INFO: Loading: sentiment\n",
      "2021-03-29 13:25:25 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en', processors='tokenize,sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Ram is a bad boy')\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(i, sentence.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Ram is a good boy')\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(i, sentence.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Ram is a boy')\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(i, sentence.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-29 13:26:36 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2021-03-29 13:26:36 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2021-03-29 13:26:36 INFO: Use device: cpu\n",
      "2021-03-29 13:26:36 INFO: Loading: tokenize\n",
      "2021-03-29 13:26:36 INFO: Loading: pos\n",
      "2021-03-29 13:26:36 INFO: Loading: lemma\n",
      "2021-03-29 13:26:36 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Akshay \tlemma: Akshay\n",
      "word: is \tlemma: be\n",
      "word: teaching \tlemma: teach\n",
      "word: Stanza \tlemma: Stanza\n",
      "word: . \tlemma: .\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
    "doc = nlp('Akshay is teaching Stanza.')\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
